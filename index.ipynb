{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages :\n",
    "\n",
    "import pandas as pd\n",
    "from main import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_nodes = pd.read_csv('input/nodes.csv')\n",
    "data_links = pd.read_csv('input/links.csv')\n",
    "\n",
    "print( type(data_nodes), '\\n', data_links.shape )\n",
    "\n",
    "nodes = dict(data_nodes)\n",
    "links = dict(data_links)\n",
    "\n",
    "Start = data.DataHandle( data_links= links, data_nodes= nodes)\n",
    "\n",
    "advertisers_nodes, publisher_nodes = Start.initial_data( need_nodes = True)\n",
    "\n",
    "layer_one_links, layer_two_links, interconnected_links = Start.modify_links()\n",
    "\n",
    "print(len(layer_one_links), '\\n', len(layer_two_links), '\\n', len(interconnected_links))\n",
    "\n",
    "print( layer_one_links[470], '\\n', layer_two_links[1000], '\\n', interconnected_links[10000] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(advertisers_nodes), len(publisher_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ruunig time of previous cell was : 4m and 16s\n",
    "#Here is the outputs!\n",
    "'''\n",
    "<class 'pandas.core.frame.DataFrame'> \n",
    "(969395, 4)\n",
    "Getting things Ready . . . \n",
    "93924 \n",
    "773939 \n",
    "101532\n",
    "(28, 1939) \n",
    "(5699, 6378) \n",
    "(590, 7377)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this to save the outputs from  modify_links  method !\n",
    "# There is one sample of this output , in Archive Folder .\n",
    "\n",
    "\n",
    "layer_one_links_save = pd.DataFrame(layer_one_links)\n",
    "layer_one_links_save.set_axis(['source', 'target'], axis=1)\n",
    "layer_one_links_save.to_csv('output/Layer One Links - GeneratedFROMindex.py.csv', index = False )\n",
    "\n",
    "layer_two_links_save = pd.DataFrame(layer_two_links)\n",
    "layer_two_links_save.set_axis(['source', 'target'], axis=1)\n",
    "layer_two_links_save.to_csv('output/Layer Two Links - GeneratedFROMindex.py.csv', index = False )\n",
    "\n",
    "interconnected_links_save = pd.DataFrame(interconnected_links)\n",
    "interconnected_links_save.set_axis(['source', 'target'], axis=1)\n",
    "interconnected_links_save.to_csv('output/InterConnected Links - GeneratedFROMindex.py.csv', index = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Will Load the data from previues saving outputs from  modify_links  method !\n",
    "\n",
    "\n",
    "layer_one_links_from_save = pd.read_csv('Archive/csv/Layer One Links - GeneratedFROMindex.py.csv')\n",
    "layer_two_links_from_save = pd.read_csv('Archive/csv/Layer Two Links - GeneratedFROMindex.py.csv')\n",
    "interconnected_links_from_save = pd.read_csv('Archive/csv/InterConnected Links - GeneratedFROMindex.py.csv')\n",
    "layer_one_links = list(layer_one_links_from_save.values.tolist())\n",
    "layer_two_links = list(layer_two_links_from_save.values.tolist())\n",
    "interconnected_links = list(interconnected_links_from_save.values.tolist())\n",
    "\n",
    "# This will return the exact result as the earlier outputs from  modify_links  method\n",
    "\n",
    "print(len(layer_one_links), '\\n', len(layer_two_links), '\\n', len(interconnected_links),\n",
    "      layer_one_links[470], '\\n', layer_two_links[1000], '\\n', interconnected_links[10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That was too much!(969395)! We selected some of the nodes and links base on their likes in R before !!\n",
    "# Loading that data to avoid long proccess and time !\n",
    "\n",
    "new_data = pd.read_csv( 'input/link_dataFinal.csv' , index_col=0)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the nodes from new_data to pass to the DataHandel class!\n",
    "\n",
    "new_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data ready by turn it into dict !\n",
    "\n",
    "new_nodes = []\n",
    "\n",
    "for node in new_data.loc[:,'source'].unique() :\n",
    "    if node not in new_nodes :\n",
    "        new_nodes.append(node)\n",
    "\n",
    "for node in new_data.loc[:,'target'].unique() :\n",
    "    if node not in new_nodes :\n",
    "        new_nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue getting data ready this time by turnnig to dicts both !\n",
    "\n",
    "new_links = dict(new_data)\n",
    "new_nodes = pd.DataFrame(new_nodes)\n",
    "new_nodes = new_nodes.set_axis(['id'], axis=1)\n",
    "new_nodes = dict(new_nodes)\n",
    "print(type(new_nodes), type(new_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will prepare and preprocess the data :\n",
    "\n",
    "Start = data.DataHandle( data_links= new_links, data_nodes= new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start.initial_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method finding the source and target palce and values in our data and Also\n",
    "# find and initial the values for each layer !\n",
    "\n",
    "#Start.initial_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error seggest to us provide adv and pub nodes manually. This is beacuse our new_data\n",
    "# had no obvious intialize laybel for whether a node is represent the advertisers or publishers\n",
    "\n",
    "# So we need to pass these argumans to initial method from DataHandel class :\n",
    "#            adertiser_nodes= ??? , publisher_nodes = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertiser_campaigns = pd.read_csv( 'input/advertiser_campaigns.csv' , index_col=0)\n",
    "publisher_contents = pd.read_csv( 'input/publisher_contents.csv' , index_col=0)\n",
    "print(advertiser_campaigns.columns[13], '\\n',publisher_contents.columns[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertisers_nodes = list( advertiser_campaigns['Advertiser ID'].unique() )#It must be list\n",
    "publishers_nodes = list( publisher_contents['Media App Media - Media â†’ Publisher ID'].unique() )#It must be list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again using our two new lists:\n",
    "\n",
    "Start.initial_data( advertiser_nodes = advertisers_nodes , publisher_nodes = publishers_nodes )\n",
    "\n",
    "# WorKed :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer_one_links, new_layer_two_links, new_interconnected_links = Start.modify_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(new_layer_one_links), len(new_layer_two_links), len(new_interconnected_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graph.Graph(layer_one_name= 'Advertisers', layer_two_name= 'Publishers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = G.add_links( layer_one_links = layer_one_links ,\n",
    "            layer_two_links = layer_two_links ,\n",
    "            Interconnected_links = interconnected_links )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pymnet = G.add_links( layer_one_links = new_layer_one_links ,\n",
    "            layer_two_links = new_layer_two_links ,\n",
    "            Interconnected_links = new_interconnected_links )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Big Data\n",
    "\n",
    "g_nx_layer1, g_nx_layer2 , g_nx = G.add_links( layer_one_links = layer_one_links ,\n",
    "                                               layer_two_links = layer_two_links ,\n",
    "                                               Interconnected_links = interconnected_links ,\n",
    "                                               nx_use = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another use of add_links method for Big Data :\n",
    "g_nx = G.add_links( layer_one_links = layer_one_links ,\n",
    "                                               layer_two_links = layer_two_links ,\n",
    "                                               Interconnected_links = interconnected_links ,\n",
    "                                               layer_one_nodes= advertisers_nodes,\n",
    "                                               layer_two_nodes= publisher_nodes,\n",
    "                                               nx_use = True,\n",
    "                                               just_interconnect_for_nx_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another use of add_links method for simple data :\n",
    "g_nx = G.add_links( layer_one_links = new_layer_one_links ,\n",
    "                                               layer_two_links = new_layer_two_links ,\n",
    "                                               Interconnected_links = new_interconnected_links ,\n",
    "                                               layer_one_nodes= advertisers_nodes,\n",
    "                                               layer_two_nodes= publishers_nodes,\n",
    "                                               nx_use = True,\n",
    "                                               just_interconnect_for_nx_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if you are using simple data\n",
    "\n",
    "g_nx_layer1, g_nx_layer2 , g_nx = G.add_links( layer_one_links = new_layer_one_links ,\n",
    "                                               layer_two_links = new_layer_two_links ,\n",
    "                                               Interconnected_links = new_interconnected_links ,\n",
    "                                               nx_use = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_pymnet , '\\n', g_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a few while :\n",
    "\n",
    "Matrix_from_g , Lists_from_g = g_pymnet.get_supra_adjacency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "with open('output/matrix of G.pkl', 'wb') as file:\n",
    "    pickle.dump(Matrix_from_g, file)\n",
    "\n",
    "with open('output/list of G.pkl', 'wb') as file:\n",
    "    pickle.dump(Lists_from_g, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as previous cell But for All Data set \n",
    "# This can Be find in Archive !\n",
    "\n",
    "'''\n",
    "Matrix_from_gBigData , Lists_from_gBigData = g.get_supra_adjacency_matrix()\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('output/matrix of G BigData.pkl', 'wb') as file:\n",
    "    pickle.dump(Matrix_from_gBigData, file)\n",
    "\n",
    "with open('output/list of G BigData.pkl', 'wb') as file:\n",
    "    pickle.dump(Lists_from_gBigData, file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.show(g_pymnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality Measures :\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_centrality_measures(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    return degree_centrality, betweenness_centrality, closeness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# Graph\n",
    "G = g_nx\n",
    "\n",
    "degree_centrality, betweenness_centrality, closeness_centrality = calculate_centrality_measures(G)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Calculating features for each pair of nodes:\n",
    "for node1, node2 in combinations(G.nodes(), 2):\n",
    "    \n",
    "    # Centrality measures for node1 and node2\n",
    "    degree_centrality_1 = degree_centrality[node1]\n",
    "    degree_centrality_2 = degree_centrality[node2]\n",
    "    betweenness_centrality_1 = betweenness_centrality[node1]\n",
    "    betweenness_centrality_2 = betweenness_centrality[node2]\n",
    "    closeness_centrality_1 = closeness_centrality[node1]\n",
    "    closeness_centrality_2 = closeness_centrality[node2]\n",
    "    \n",
    "    # Directly compute and append features    \n",
    "    features.append((\n",
    "        node1,\n",
    "        node2,\n",
    "        len(list(nx.common_neighbors(G, node1, node2))),  # Common Neighbors\n",
    "        next(nx.preferential_attachment(G, [(node1, node2)]))[2],  # Preferential Attachment\n",
    "        next(nx.resource_allocation_index(G, [(node1, node2)]))[2],  # Resource Allocation\n",
    "        next(nx.adamic_adar_index(G, [(node1, node2)]))[2],  # Adamic Adar\n",
    "        next(nx.jaccard_coefficient(G, [(node1, node2)]))[2] , # Jaccard Coefficient\n",
    "        (degree_centrality_1 + degree_centrality_2) / 2,\n",
    "        (betweenness_centrality_1 + betweenness_centrality_2) / 2,\n",
    "        (closeness_centrality_1 + closeness_centrality_2) / 2\n",
    "    ))\n",
    "\n",
    "    # Existence of links between nodes (1 or 0)\n",
    "    labels.append(1 if G.has_edge(node1, node2) else 0)\n",
    "\n",
    "# Creating a DataFrame from the features and labels\n",
    "features_df = pd.DataFrame(features,\n",
    "                           columns=['node1','node2','Common Neighbors', \n",
    "                                    'Preferential Attachment', 'Resource Allocation', \n",
    "                                    'Adamic Adar', 'Jaccard Coefficient', 'Average Degree Centrality',\n",
    "                                    'Average Betweenness Centrality', 'Average Closeness Centrality'])\n",
    "\n",
    "features_df['Label'] = labels\n",
    "\n",
    "# Now features_df is ready to use in a machine learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_features_df = pd.read_csv('Archive/csv/features_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Graph\n",
    "G = g_nx\n",
    "\n",
    "# Precomputed centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Precompute values to avoid repeated calculations\n",
    "common_neighbors = {pair: len(list(nx.common_neighbors(G, *pair))) for pair in combinations(G.nodes(), 2)}\n",
    "preferential_attachment = {pair: next(val) for pair, val in nx.preferential_attachment(G)}\n",
    "resource_allocation_index = {pair: next(val) for pair, val in nx.resource_allocation_index(G)}\n",
    "adamic_adar_index = {pair: next(val) for pair, val in nx.adamic_adar_index(G)}\n",
    "jaccard_coefficient = {pair: next(val) for pair, val in nx.jaccard_coefficient(G)}\n",
    "\n",
    "# Calculating features for each pair of nodes\n",
    "for node1, node2 in combinations(G.nodes(), 2):\n",
    "    features.append((\n",
    "        node1,\n",
    "        node2,\n",
    "        common_neighbors[(node1, node2)],\n",
    "        preferential_attachment[(node1, node2)][2],\n",
    "        resource_allocation_index[(node1, node2)][2],\n",
    "        adamic_adar_index[(node1, node2)][2],\n",
    "        jaccard_coefficient[(node1, node2)][2],\n",
    "        (degree_centrality[node1] + degree_centrality[node2]) / 2,\n",
    "        (betweenness_centrality[node1] + betweenness_centrality[node2]) / 2,\n",
    "        (closeness_centrality[node1] + closeness_centrality[node2]) / 2,\n",
    "    ))\n",
    "\n",
    "    labels.append(1 if G.has_edge(node1, node2) else 0)\n",
    "\n",
    "# Creating a DataFrame from the features and labels\n",
    "features_df = pd.DataFrame(features,\n",
    "                           columns=['node1', 'node2', 'Common Neighbors', \n",
    "                                    'Preferential Attachment', 'Resource Allocation', \n",
    "                                    'Adamic Adar', 'Jaccard Coefficient', 'Average Degree Centrality',\n",
    "                                    'Average Betweenness Centrality', 'Average Closeness Centrality'])\n",
    "\n",
    "features_df['Label'] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = link.LinkPrediction(g_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Python/mrSina/main/link.py:-1\u001b[0m, in \u001b[0;36mLinkPrediction.train_model\u001b[0;34m(self, methods)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "A.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your Graph created with big data use this to save the extracted features :\n",
    "\n",
    "features_df.to_csv('output/Big Data features DataFrame.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if you working with simple dataset\n",
    "\n",
    "features_df.to_csv('output/features_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Assuming features_df is your DataFrame from the previous step\n",
    "X = old_features_df.drop(['node1','node2','Label'], axis=1)  # Features\n",
    "y = old_features_df['Label']  # Labels\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "print(f\"True Negatives (TN): {conf_matrix[0][0]} pairs were correctly predicted as not having a link.\")\n",
    "    \n",
    "print(f\"False Positives (FP): {conf_matrix[0][1]} pairs were incorrectly predicted as having a link (but actually do not).\")\n",
    "\n",
    "print(f\"False Negatives (FN): {conf_matrix[1][0]} pairs were incorrectly predicted as not having a link (but they actually do).\")\n",
    "    \n",
    "print(f\"True Positives (TP): {conf_matrix[1][1]} pairs were correctly predicted as having a link.\")\n",
    "\n",
    "# Example of making a prediction for a new pair of nodes\n",
    "# Let's say we want to predict the likelihood of a link between two new nodes\n",
    "# with the following features: [5 (common neighbors), 10 (preferential attachment),\n",
    "# 0.15 (resource allocation), 0.5 (adamic adar), 0.2 (jaccard coefficient)]\n",
    "#new_pair_features = [[5, 10, 0.15, 0.5, 0.2]]\n",
    "#new_pair_prediction = rf_classifier.predict(new_pair_features)\n",
    "#print(f'Prediction for the new pair: {new_pair_prediction[0]}')\n",
    "# A prediction of 1 indicates a likely link, while 0 indicates unlikely.\n",
    "\n",
    "# Confusion Matrix Guide :\n",
    "#    [ [TN    FP]\n",
    "#      [FN    TP] ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This was Before adding calculate_centrality_measures ::\n",
    "\n",
    "    True Negatives (TN): 26178 pairs were correctly predicted as not having a link.\n",
    "    \n",
    "    False Positives (FP): 14 pairs were incorrectly predicted as having a link (but actually do not).\n",
    "    \n",
    "    False Negatives (FN): 69 pairs were incorrectly predicted as not having a link (but they actually do).\n",
    "    \n",
    "    True Positives (TP): 727 pairs were correctly predicted as having a link.\n",
    "\n",
    "\n",
    "# After:\n",
    "    [[26181    11]\n",
    "    [   54   742]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "X = np.array(features_df.drop(['node1','node2','Label'], axis=1))\n",
    "y = np.array(features_df['Label'])\n",
    "\n",
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=2)\n",
    "scores = []\n",
    "confuse_matrix = []\n",
    "\n",
    "# To store predictions and actual labels\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train model\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42, verbose = 1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probabilities = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Adjust threshold here if needed :\n",
    "    threshold = 0.5\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, predictions, average='binary')\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    confuse_matrix.append(confusion_matrix(y_test, predictions))\n",
    "    all_predictions.extend(predictions)\n",
    "    all_actuals.extend(y_test)\n",
    "    scores.append((accuracy, precision, recall, fscore))\n",
    "\n",
    "\n",
    "# Creating DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': all_actuals,\n",
    "    'Predicted': all_predictions\n",
    "})\n",
    "\n",
    "# Saving DataFrame to CSV\n",
    "csv_path = 'output/predictions_and_actuals.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions and actual labels to {csv_path}\")\n",
    "\n",
    "# Calculate average scores\n",
    "avg_scores = np.mean(scores, axis=0)\n",
    "print(f\"Average Accuracy: {avg_scores[0]}\")\n",
    "print(f\"Average Precision: {avg_scores[1]}\")\n",
    "print(f\"Average Recall: {avg_scores[2]}\")\n",
    "print(f\"Average F1-Score: {avg_scores[3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Guide :\n",
    "#    [ [TN    FP]\n",
    "#      [FN    TP] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we look at the Confusion Matrix :\n",
    "\n",
    "tn_fn = []\n",
    "fp_tp = []\n",
    "for TNFP_FNTP in confuse_matrix:\n",
    "    \n",
    "    for TN_and_FN_or_FP_and_TP in TNFP_FNTP :\n",
    "        TN_and_FN = TN_and_FN_or_FP_and_TP[0]\n",
    "        FP_and_TP = TN_and_FN_or_FP_and_TP[1]\n",
    "        tn_fn.append(TN_and_FN)\n",
    "        fp_tp.append(FP_and_TP)\n",
    "tn = []\n",
    "fn = []\n",
    "fp = []\n",
    "tp = []\n",
    "for i in range(0, len(tn_fn), 2) :\n",
    "    tn.append(tn_fn[i])\n",
    "for i in range(1, len(tn_fn), 2) :\n",
    "    fn.append(tn_fn[i])\n",
    "for i in range(0, len(tn_fn), 2) :\n",
    "    fp.append(fp_tp[i])\n",
    "for i in range(1, len(tn_fn), 2) :\n",
    "    tp.append(fp_tp[i])\n",
    "\n",
    "print(f\" The Max True Negatives (TN): {max(tn)} \\n\",\n",
    "      f\"The min False Negatives(FN): {min(fn)} \\n\",\n",
    "      f\"The min False Positives(FP): {min(fp)} \\n\",\n",
    "      f\"The Max True Positives (TP): {max(tp)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_nx(G, title =  None, save_ = False, save_to = \"output\"):\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    centrality = nx.betweenness_centrality(G)\n",
    "    nx.draw(G, pos, node_size=[v * 10000 for v in centrality.values()], node_color=list(centrality.values()), cmap=plt.cm.viridis, with_labels=False)\n",
    "    plt.title(title)\n",
    "    if save_ == True:\n",
    "        if title is not None :\n",
    "            plt.savefig(f\"{save_to}/Fig {title} from visualize_nx.png\")\n",
    "        else:\n",
    "            plt.savefig(f\"{save_to}/Fig{np.random.randint(10000)} from visualize_nx.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_nx(g_nx, title = 'g_nx', save_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_nx(g_nx_layer1, title = 'g_nx_layer1', save_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_nx(g_nx_layer2, title = 'g_nx_layer2', save_ = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gragh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
